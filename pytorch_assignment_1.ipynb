{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\anaconda3\\envs\\pytorch_nn_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleonard-puskac\u001b[0m (\u001b[33mfiit-nn-2023-lp-vs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Leonard\\Desktop\\2.semester\\NN\\NN_assignment_1\\wandb\\run-20230330_163714-8lr8unyd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1/runs/8lr8unyd' target=\"_blank\">denim-snowflake-15</a></strong> to <a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1/runs/8lr8unyd' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1/runs/8lr8unyd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1/runs/8lr8unyd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x19d25859940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"nn-assignment-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sonar.all-data\") as all_data_file:\n",
    "    lines = all_data_file.readlines()\n",
    "    all_data = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split(',')\n",
    "        label = line.pop()\n",
    "        #line = np.asarray(line, dtype=float)\n",
    "        if label == \"R\":\n",
    "            labels.append(0)\n",
    "            all_data.append(line)\n",
    "        elif label == \"M\":\n",
    "            labels.append(1)\n",
    "            all_data.append(line)\n",
    "        else:\n",
    "            pass\n",
    "    all_data = np.asarray(all_data, dtype=float)\n",
    "    labels = np.asarray(labels, dtype=float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalization, dropping columns with correlation lower than 0.1 with the resulting class, and spliting into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_2darray(d2array):\n",
    "    output_array = []\n",
    "    for array in d2array:\n",
    "        x = (array - np.mean(array)) / np.std(array)\n",
    "        #x[x<0] *= -1\n",
    "        #x = (x-np.min(x))/(np.max(x) - np.min(x))\n",
    "        output_array.append(x)\n",
    "    return np.asarray(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_2(input_data, input_labels):\n",
    "    indices = np.random.permutation(input_data.shape[0])\n",
    "    split_idx = math.floor(input_data.shape[0] * 0.7)\n",
    "    train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
    "    train_data, test_data = input_data[train_idx,:], input_data[test_idx,:]\n",
    "    train_labels, test_labels = input_labels[train_idx], input_labels[test_idx]\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = normalise_2darray(all_data)\n",
    "\n",
    "corr_arr = []\n",
    "for idx, row in enumerate(x_norm):\n",
    "    arr = np.asarray(list(row) + [labels[idx]])\n",
    "    corr_arr.append(arr)\n",
    "corr_arr = np.asarray(corr_arr)\n",
    "corr_map = np.corrcoef(corr_arr, rowvar=False).round(2)\n",
    "corr_map = corr_map[:, 60]  # keep only final column of the heatmap | correlation to target class\n",
    "corr_map = corr_map.reshape((61, 1))\n",
    "\n",
    "to_drop = []\n",
    "for idx, value in enumerate(corr_map):\n",
    "    if value > -0.1 and value < 0.1:\n",
    "        to_drop.append(idx)\n",
    "\n",
    "x_norm = np.delete(x_norm, to_drop, axis=1)\n",
    "x_norm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 37) (63, 37) (145,) (63,)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#x_train, x_val = train_test_split(x_norm, test_size=0.3)\n",
    "#y_train, y_val = train_test_split(labels, test_size=0.3)\n",
    "x_train, x_val, y_train, y_val = train_test_split_2(x_norm, labels)\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up \"device\", converting the numpy arrays into torch tensors and passing them to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145, 37]) torch.Size([63, 37]) torch.Size([145, 1]) torch.Size([63, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "x_val = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device).reshape(-1,1).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device).reshape(-1,1).to(device)\n",
    "\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.network = self.__setup_network()\n",
    "\n",
    "    def __setup_network(self):\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        return net\n",
    "    def forward(self, input):\n",
    "        return self.network(input)\n",
    "    \n",
    "    def fit(self, X, y, X_val, y_val, n_epochs, loss_fn, optimizer, batch_size=30):\n",
    "        batches = torch.arange(0, len(X), batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            best_loss = 20000\n",
    "            best_acc = 0\n",
    "            self.train()\n",
    "            for batch_start in batches:\n",
    "                X_batch = X[batch_start:(batch_start+batch_size)]\n",
    "                y_batch = y[batch_start:(batch_start+batch_size)]\n",
    "\n",
    "                prediction = self.forward(X_batch)\n",
    "                loss = loss_fn(prediction, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                acc = (prediction.round() == y_batch).float().mean()\n",
    "                #wandb.log({\"loss\": loss})\n",
    "                #wandb.log({\"acc\": acc})\n",
    "                best_acc = acc if acc > best_acc else best_acc\n",
    "                best_loss = loss if loss < best_loss else best_loss\n",
    "            wandb.log({\"loss\": best_loss})\n",
    "            wandb.log({\"acc\": best_acc})\n",
    "            \n",
    "            self.eval()\n",
    "            y_pred = self.network(X_val)\n",
    "            val_acc = (y_pred.round() == y_val).float().mean()\n",
    "            val_loss = loss_fn(y_pred, y_val)\n",
    "            wandb.log({\"val_acc\": val_acc})\n",
    "            print(f\"Epoch {epoch}:\\ttrain_loss: {best_loss:.4f}\\ttrain_acc: {best_acc:.4f}\\tval_loss: {val_loss:.4f}\\tval_acc: {val_acc:.4f}\")\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\ttrain_loss: 0.6922\ttrain_acc: 0.5333\tval_loss: 0.6940\tval_acc: 0.5397\n",
      "Epoch 1:\ttrain_loss: 0.6848\ttrain_acc: 0.5667\tval_loss: 0.6929\tval_acc: 0.5397\n",
      "Epoch 2:\ttrain_loss: 0.6839\ttrain_acc: 0.5667\tval_loss: 0.6880\tval_acc: 0.5397\n",
      "Epoch 3:\ttrain_loss: 0.6122\ttrain_acc: 0.8333\tval_loss: 0.6910\tval_acc: 0.6349\n",
      "Epoch 4:\ttrain_loss: 0.4273\ttrain_acc: 0.9000\tval_loss: 0.6229\tval_acc: 0.7143\n",
      "Epoch 5:\ttrain_loss: 0.3169\ttrain_acc: 0.9000\tval_loss: 0.6389\tval_acc: 0.7143\n",
      "Epoch 6:\ttrain_loss: 0.4153\ttrain_acc: 0.8333\tval_loss: 0.6228\tval_acc: 0.6825\n",
      "Epoch 7:\ttrain_loss: 0.3975\ttrain_acc: 0.9000\tval_loss: 0.6146\tval_acc: 0.6825\n",
      "Epoch 8:\ttrain_loss: 0.3920\ttrain_acc: 0.9000\tval_loss: 0.5846\tval_acc: 0.7143\n",
      "Epoch 9:\ttrain_loss: 0.3704\ttrain_acc: 0.9000\tval_loss: 0.6296\tval_acc: 0.6984\n",
      "Epoch 10:\ttrain_loss: 0.3533\ttrain_acc: 0.8667\tval_loss: 0.6135\tval_acc: 0.6667\n",
      "Epoch 11:\ttrain_loss: 0.3636\ttrain_acc: 0.9000\tval_loss: 0.5897\tval_acc: 0.6825\n",
      "Epoch 12:\ttrain_loss: 0.3207\ttrain_acc: 0.9000\tval_loss: 0.5661\tval_acc: 0.7302\n",
      "Epoch 13:\ttrain_loss: 0.3247\ttrain_acc: 0.9000\tval_loss: 0.5335\tval_acc: 0.7619\n",
      "Epoch 14:\ttrain_loss: 0.2808\ttrain_acc: 0.9333\tval_loss: 0.6152\tval_acc: 0.7143\n",
      "Epoch 15:\ttrain_loss: 0.3088\ttrain_acc: 0.9000\tval_loss: 0.5658\tval_acc: 0.7460\n",
      "Epoch 16:\ttrain_loss: 0.3205\ttrain_acc: 0.9000\tval_loss: 0.6369\tval_acc: 0.6825\n",
      "Epoch 17:\ttrain_loss: 0.3168\ttrain_acc: 0.9000\tval_loss: 0.6013\tval_acc: 0.7143\n",
      "Epoch 18:\ttrain_loss: 0.2761\ttrain_acc: 0.9667\tval_loss: 0.5492\tval_acc: 0.7619\n",
      "Epoch 19:\ttrain_loss: 0.3755\ttrain_acc: 0.8667\tval_loss: 0.5894\tval_acc: 0.7302\n",
      "Epoch 20:\ttrain_loss: 0.2822\ttrain_acc: 0.9000\tval_loss: 0.5501\tval_acc: 0.7778\n",
      "Epoch 21:\ttrain_loss: 0.2755\ttrain_acc: 0.9333\tval_loss: 0.5600\tval_acc: 0.7778\n",
      "Epoch 22:\ttrain_loss: 0.2742\ttrain_acc: 0.9333\tval_loss: 0.5519\tval_acc: 0.7619\n",
      "Epoch 23:\ttrain_loss: 0.2674\ttrain_acc: 0.9333\tval_loss: 0.5197\tval_acc: 0.7778\n",
      "Epoch 24:\ttrain_loss: 0.2363\ttrain_acc: 0.9667\tval_loss: 0.5333\tval_acc: 0.7619\n",
      "Epoch 25:\ttrain_loss: 0.2497\ttrain_acc: 0.9333\tval_loss: 0.5686\tval_acc: 0.7778\n",
      "Epoch 26:\ttrain_loss: 0.3421\ttrain_acc: 0.9200\tval_loss: 0.5386\tval_acc: 0.7460\n",
      "Epoch 27:\ttrain_loss: 0.2316\ttrain_acc: 0.9667\tval_loss: 0.5643\tval_acc: 0.7460\n",
      "Epoch 28:\ttrain_loss: 0.2365\ttrain_acc: 0.9667\tval_loss: 0.5613\tval_acc: 0.7460\n",
      "Epoch 29:\ttrain_loss: 0.2330\ttrain_acc: 0.9667\tval_loss: 0.5603\tval_acc: 0.7619\n",
      "Epoch 30:\ttrain_loss: 0.2290\ttrain_acc: 0.9667\tval_loss: 0.6146\tval_acc: 0.7302\n",
      "Epoch 31:\ttrain_loss: 0.3384\ttrain_acc: 0.9200\tval_loss: 0.5615\tval_acc: 0.7778\n",
      "Epoch 32:\ttrain_loss: 0.2301\ttrain_acc: 0.9667\tval_loss: 0.5708\tval_acc: 0.7460\n",
      "Epoch 33:\ttrain_loss: 0.2165\ttrain_acc: 0.9667\tval_loss: 0.5972\tval_acc: 0.7619\n",
      "Epoch 34:\ttrain_loss: 0.2290\ttrain_acc: 0.9333\tval_loss: 0.5908\tval_acc: 0.7619\n",
      "Epoch 35:\ttrain_loss: 0.2984\ttrain_acc: 0.9200\tval_loss: 0.5448\tval_acc: 0.7460\n",
      "Epoch 36:\ttrain_loss: 0.2057\ttrain_acc: 0.9667\tval_loss: 0.5599\tval_acc: 0.7619\n",
      "Epoch 37:\ttrain_loss: 0.1796\ttrain_acc: 0.9667\tval_loss: 0.5527\tval_acc: 0.7778\n",
      "Epoch 38:\ttrain_loss: 0.2926\ttrain_acc: 0.9200\tval_loss: 0.5543\tval_acc: 0.7619\n",
      "Epoch 39:\ttrain_loss: 0.1556\ttrain_acc: 1.0000\tval_loss: 0.5804\tval_acc: 0.7460\n",
      "Epoch 40:\ttrain_loss: 0.1405\ttrain_acc: 1.0000\tval_loss: 0.5508\tval_acc: 0.7619\n",
      "Epoch 41:\ttrain_loss: 0.1444\ttrain_acc: 0.9667\tval_loss: 0.6283\tval_acc: 0.7619\n",
      "Epoch 42:\ttrain_loss: 0.1407\ttrain_acc: 1.0000\tval_loss: 0.5972\tval_acc: 0.7460\n",
      "Epoch 43:\ttrain_loss: 0.1385\ttrain_acc: 1.0000\tval_loss: 0.6758\tval_acc: 0.7302\n",
      "Epoch 44:\ttrain_loss: 0.1829\ttrain_acc: 0.9667\tval_loss: 0.6127\tval_acc: 0.7619\n",
      "Epoch 45:\ttrain_loss: 0.2343\ttrain_acc: 0.9333\tval_loss: 0.5748\tval_acc: 0.7619\n",
      "Epoch 46:\ttrain_loss: 0.1357\ttrain_acc: 1.0000\tval_loss: 0.5980\tval_acc: 0.7778\n",
      "Epoch 47:\ttrain_loss: 0.1917\ttrain_acc: 0.9333\tval_loss: 0.5697\tval_acc: 0.7460\n",
      "Epoch 48:\ttrain_loss: 0.1322\ttrain_acc: 1.0000\tval_loss: 0.5940\tval_acc: 0.7937\n",
      "Epoch 49:\ttrain_loss: 0.2425\ttrain_acc: 0.9333\tval_loss: 0.5900\tval_acc: 0.7937\n",
      "Epoch 50:\ttrain_loss: 0.1565\ttrain_acc: 0.9667\tval_loss: 0.6065\tval_acc: 0.7619\n",
      "Epoch 51:\ttrain_loss: 0.1185\ttrain_acc: 1.0000\tval_loss: 0.6048\tval_acc: 0.7778\n",
      "Epoch 52:\ttrain_loss: 0.2194\ttrain_acc: 0.9600\tval_loss: 0.5951\tval_acc: 0.7778\n",
      "Epoch 53:\ttrain_loss: 0.2054\ttrain_acc: 0.9600\tval_loss: 0.5996\tval_acc: 0.7937\n",
      "Epoch 54:\ttrain_loss: 0.2492\ttrain_acc: 0.9200\tval_loss: 0.5916\tval_acc: 0.7778\n",
      "Epoch 55:\ttrain_loss: 0.1587\ttrain_acc: 0.9667\tval_loss: 0.5939\tval_acc: 0.7937\n",
      "Epoch 56:\ttrain_loss: 0.1825\ttrain_acc: 0.9667\tval_loss: 0.5984\tval_acc: 0.7937\n",
      "Epoch 57:\ttrain_loss: 0.2117\ttrain_acc: 0.9600\tval_loss: 0.6274\tval_acc: 0.7619\n",
      "Epoch 58:\ttrain_loss: 0.2099\ttrain_acc: 0.9600\tval_loss: 0.6236\tval_acc: 0.7619\n",
      "Epoch 59:\ttrain_loss: 0.2596\ttrain_acc: 0.9333\tval_loss: 0.6093\tval_acc: 0.7619\n",
      "Epoch 60:\ttrain_loss: 0.1144\ttrain_acc: 1.0000\tval_loss: 0.5992\tval_acc: 0.7778\n",
      "Epoch 61:\ttrain_loss: 0.1641\ttrain_acc: 0.9667\tval_loss: 0.6114\tval_acc: 0.7778\n",
      "Epoch 62:\ttrain_loss: 0.2164\ttrain_acc: 0.9600\tval_loss: 0.5964\tval_acc: 0.7778\n",
      "Epoch 63:\ttrain_loss: 0.2163\ttrain_acc: 0.9600\tval_loss: 0.6087\tval_acc: 0.7778\n",
      "Epoch 64:\ttrain_loss: 0.1644\ttrain_acc: 0.9667\tval_loss: 0.5935\tval_acc: 0.7937\n",
      "Epoch 65:\ttrain_loss: 0.1073\ttrain_acc: 1.0000\tval_loss: 0.6119\tval_acc: 0.7778\n",
      "Epoch 66:\ttrain_loss: 0.2203\ttrain_acc: 0.9600\tval_loss: 0.6038\tval_acc: 0.7778\n",
      "Epoch 67:\ttrain_loss: 0.1106\ttrain_acc: 1.0000\tval_loss: 0.6328\tval_acc: 0.7302\n",
      "Epoch 68:\ttrain_loss: 0.1347\ttrain_acc: 1.0000\tval_loss: 0.6158\tval_acc: 0.7619\n",
      "Epoch 69:\ttrain_loss: 0.2619\ttrain_acc: 0.9200\tval_loss: 0.6342\tval_acc: 0.7302\n",
      "Epoch 70:\ttrain_loss: 0.1233\ttrain_acc: 1.0000\tval_loss: 0.6312\tval_acc: 0.7460\n",
      "Epoch 71:\ttrain_loss: 0.1149\ttrain_acc: 1.0000\tval_loss: 0.6425\tval_acc: 0.7778\n",
      "Epoch 72:\ttrain_loss: 0.2018\ttrain_acc: 0.9333\tval_loss: 0.5992\tval_acc: 0.7302\n",
      "Epoch 73:\ttrain_loss: 0.1376\ttrain_acc: 1.0000\tval_loss: 0.6335\tval_acc: 0.7619\n",
      "Epoch 74:\ttrain_loss: 0.2351\ttrain_acc: 0.9600\tval_loss: 0.6168\tval_acc: 0.7778\n",
      "Epoch 75:\ttrain_loss: 0.1237\ttrain_acc: 0.9667\tval_loss: 0.6610\tval_acc: 0.7460\n",
      "Epoch 76:\ttrain_loss: 0.1346\ttrain_acc: 0.9667\tval_loss: 0.6537\tval_acc: 0.7778\n",
      "Epoch 77:\ttrain_loss: 0.1465\ttrain_acc: 0.9667\tval_loss: 0.6549\tval_acc: 0.7619\n",
      "Epoch 78:\ttrain_loss: 0.1073\ttrain_acc: 1.0000\tval_loss: 0.6797\tval_acc: 0.7460\n",
      "Epoch 79:\ttrain_loss: 0.0916\ttrain_acc: 1.0000\tval_loss: 0.7306\tval_acc: 0.7460\n",
      "Epoch 80:\ttrain_loss: 0.2197\ttrain_acc: 0.9600\tval_loss: 0.6115\tval_acc: 0.7778\n",
      "Epoch 81:\ttrain_loss: 0.1152\ttrain_acc: 1.0000\tval_loss: 0.6773\tval_acc: 0.7619\n",
      "Epoch 82:\ttrain_loss: 0.0971\ttrain_acc: 1.0000\tval_loss: 0.6690\tval_acc: 0.7619\n",
      "Epoch 83:\ttrain_loss: 0.2171\ttrain_acc: 0.9600\tval_loss: 0.6583\tval_acc: 0.7460\n",
      "Epoch 84:\ttrain_loss: 0.1159\ttrain_acc: 1.0000\tval_loss: 0.8981\tval_acc: 0.7143\n",
      "Epoch 85:\ttrain_loss: 0.2130\ttrain_acc: 0.9600\tval_loss: 0.5955\tval_acc: 0.7460\n",
      "Epoch 86:\ttrain_loss: 0.1418\ttrain_acc: 0.9667\tval_loss: 0.6267\tval_acc: 0.7619\n",
      "Epoch 87:\ttrain_loss: 0.1137\ttrain_acc: 1.0000\tval_loss: 0.6119\tval_acc: 0.7619\n",
      "Epoch 88:\ttrain_loss: 0.1892\ttrain_acc: 0.9667\tval_loss: 0.6191\tval_acc: 0.7460\n",
      "Epoch 89:\ttrain_loss: 0.1029\ttrain_acc: 1.0000\tval_loss: 0.6571\tval_acc: 0.7302\n",
      "Epoch 90:\ttrain_loss: 0.2014\ttrain_acc: 0.9600\tval_loss: 0.6436\tval_acc: 0.7619\n",
      "Epoch 91:\ttrain_loss: 0.0965\ttrain_acc: 1.0000\tval_loss: 0.6413\tval_acc: 0.7778\n",
      "Epoch 92:\ttrain_loss: 0.2153\ttrain_acc: 0.9600\tval_loss: 0.6431\tval_acc: 0.7460\n",
      "Epoch 93:\ttrain_loss: 0.1053\ttrain_acc: 1.0000\tval_loss: 0.6361\tval_acc: 0.7460\n",
      "Epoch 94:\ttrain_loss: 0.1967\ttrain_acc: 0.9600\tval_loss: 0.6043\tval_acc: 0.7619\n",
      "Epoch 95:\ttrain_loss: 0.2580\ttrain_acc: 0.9333\tval_loss: 0.6273\tval_acc: 0.7460\n",
      "Epoch 96:\ttrain_loss: 0.1479\ttrain_acc: 0.9667\tval_loss: 0.5844\tval_acc: 0.7619\n",
      "Epoch 97:\ttrain_loss: 0.1192\ttrain_acc: 1.0000\tval_loss: 0.6024\tval_acc: 0.7778\n",
      "Epoch 98:\ttrain_loss: 0.1019\ttrain_acc: 1.0000\tval_loss: 0.6405\tval_acc: 0.7619\n",
      "Epoch 99:\ttrain_loss: 0.1052\ttrain_acc: 1.0000\tval_loss: 0.6367\tval_acc: 0.7460\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Network(input_size=x_train.shape[1], output_size=1).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model.network.parameters(), lr=0.0025, alpha=0.8)\n",
    "model.train()\n",
    "wandb.watch(model)\n",
    "model.fit(X=x_train, y=y_train, X_val=x_val, y_val=y_val, n_epochs=100, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
