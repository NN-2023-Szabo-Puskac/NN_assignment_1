{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\anaconda3\\envs\\pytorch_nn_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleonard-puskac\u001b[0m (\u001b[33mfiit-nn-2023-lp-vs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Leonard\\Desktop\\2.semester\\NN\\NN_assignment_1\\wandb\\run-20230330_180231-u0fyysg2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo/runs/u0fyysg2' target=\"_blank\">toasty-terrain-6</a></strong> to <a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo/runs/u0fyysg2' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo/runs/u0fyysg2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fiit-nn-2023-lp-vs/nn-assignment-1-pytorch-ooo/runs/u0fyysg2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2a3af566e80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"nn-assignment-1-pytorch-ooo\", group=\"epochs=100, batch_size=25, lr=0.001, network=ReLU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sonar.all-data\") as all_data_file:\n",
    "    lines = all_data_file.readlines()\n",
    "    all_data = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split(',')\n",
    "        label = line.pop()\n",
    "        #line = np.asarray(line, dtype=float)\n",
    "        if label == \"R\":\n",
    "            labels.append(0)\n",
    "            all_data.append(line)\n",
    "        elif label == \"M\":\n",
    "            labels.append(1)\n",
    "            all_data.append(line)\n",
    "        else:\n",
    "            pass\n",
    "    all_data = np.asarray(all_data, dtype=float)\n",
    "    labels = np.asarray(labels, dtype=float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalization, dropping columns with correlation lower than 0.1 with the resulting class, and spliting into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_2darray(d2array):\n",
    "    output_array = []\n",
    "    for array in d2array:\n",
    "        x = (array - np.mean(array)) / np.std(array)\n",
    "        #x[x<0] *= -1\n",
    "        #x = (x-np.min(x))/(np.max(x) - np.min(x))\n",
    "        output_array.append(x)\n",
    "    return np.asarray(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_2(input_data, input_labels):\n",
    "    indices = np.random.permutation(input_data.shape[0])\n",
    "    split_idx = math.floor(input_data.shape[0] * 0.7)\n",
    "    train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
    "    train_data, test_data = input_data[train_idx,:], input_data[test_idx,:]\n",
    "    train_labels, test_labels = input_labels[train_idx], input_labels[test_idx]\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = normalise_2darray(all_data)\n",
    "\n",
    "corr_arr = []\n",
    "for idx, row in enumerate(x_norm):\n",
    "    arr = np.asarray(list(row) + [labels[idx]])\n",
    "    corr_arr.append(arr)\n",
    "corr_arr = np.asarray(corr_arr)\n",
    "corr_map = np.corrcoef(corr_arr, rowvar=False).round(2)\n",
    "corr_map = corr_map[:, 60]  # keep only final column of the heatmap | correlation to target class\n",
    "corr_map = corr_map.reshape((61, 1))\n",
    "\n",
    "to_drop = []\n",
    "for idx, value in enumerate(corr_map):\n",
    "    if value > -0.1 and value < 0.1:\n",
    "        to_drop.append(idx)\n",
    "\n",
    "x_norm = np.delete(x_norm, to_drop, axis=1)\n",
    "x_norm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 37) (63, 37) (145,) (63,)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#x_train, x_val = train_test_split(x_norm, test_size=0.3)\n",
    "#y_train, y_val = train_test_split(labels, test_size=0.3)\n",
    "x_train, x_val, y_train, y_val = train_test_split_2(x_norm, labels)\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up \"device\", converting the numpy arrays into torch tensors and passing them to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145, 37]) torch.Size([63, 37]) torch.Size([145, 1]) torch.Size([63, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "x_val = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device).reshape(-1,1).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device).reshape(-1,1).to(device)\n",
    "\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.network = self.__setup_network()\n",
    "\n",
    "    def __setup_network_old(self):\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        return net\n",
    "    def __setup_network(self):\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        return net\n",
    "    def forward(self, input):\n",
    "        return self.network(input)\n",
    "    \n",
    "    def fit(self, X, y, X_val, y_val, n_epochs, loss_fn, optimizer, batch_size=30):\n",
    "        batches = torch.arange(0, len(X), batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            best_loss = 20000\n",
    "            best_acc = 0\n",
    "            self.train()\n",
    "            for batch_start in batches:\n",
    "                X_batch = X[batch_start:(batch_start+batch_size)]\n",
    "                y_batch = y[batch_start:(batch_start+batch_size)]\n",
    "\n",
    "                prediction = self.forward(X_batch)\n",
    "                loss = loss_fn(prediction, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                acc = (prediction.round() == y_batch).float().mean()\n",
    "                #wandb.log({\"loss\": loss})\n",
    "                #wandb.log({\"acc\": acc})\n",
    "                best_acc = acc if acc > best_acc else best_acc\n",
    "                best_loss = loss if loss < best_loss else best_loss\n",
    "            \n",
    "            #wandb.log({\"acc\": best_acc})\n",
    "            \n",
    "            self.eval()\n",
    "            y_pred = self.network(X_val)\n",
    "            val_acc = (y_pred.round() == y_val).float().mean()\n",
    "            val_loss = loss_fn(y_pred, y_val)\n",
    "            #wandb.log({\"val_acc\": val_acc})\n",
    "            wandb.log({\"loss\": best_loss, \"acc\": best_acc, \"val_acc\": val_acc, \"val_loss\": val_loss})\n",
    "            print(f\"Epoch {epoch}:\\ttrain_loss: {best_loss:.4f}\\ttrain_acc: {best_acc:.4f}\\tval_loss: {val_loss:.4f}\\tval_acc: {val_acc:.4f}\")\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\ttrain_loss: 0.6818\ttrain_acc: 0.7200\tval_loss: 0.6913\tval_acc: 0.5079\n",
      "Epoch 1:\ttrain_loss: 0.6658\ttrain_acc: 0.7200\tval_loss: 0.6835\tval_acc: 0.5873\n",
      "Epoch 2:\ttrain_loss: 0.6061\ttrain_acc: 0.7600\tval_loss: 0.6608\tval_acc: 0.6032\n",
      "Epoch 3:\ttrain_loss: 0.4319\ttrain_acc: 0.9000\tval_loss: 0.6501\tval_acc: 0.6349\n",
      "Epoch 4:\ttrain_loss: 0.3197\ttrain_acc: 0.9000\tval_loss: 0.6243\tval_acc: 0.6825\n",
      "Epoch 5:\ttrain_loss: 0.2904\ttrain_acc: 0.8800\tval_loss: 0.5312\tval_acc: 0.7937\n",
      "Epoch 6:\ttrain_loss: 0.2781\ttrain_acc: 0.9000\tval_loss: 0.5119\tval_acc: 0.7143\n",
      "Epoch 7:\ttrain_loss: 0.2435\ttrain_acc: 0.9200\tval_loss: 0.5576\tval_acc: 0.7778\n",
      "Epoch 8:\ttrain_loss: 0.2052\ttrain_acc: 0.9200\tval_loss: 0.5804\tval_acc: 0.7937\n",
      "Epoch 9:\ttrain_loss: 0.2137\ttrain_acc: 0.9200\tval_loss: 0.5968\tval_acc: 0.7619\n",
      "Epoch 10:\ttrain_loss: 0.1909\ttrain_acc: 0.9200\tval_loss: 0.6344\tval_acc: 0.7619\n",
      "Epoch 11:\ttrain_loss: 0.1826\ttrain_acc: 0.9500\tval_loss: 0.6639\tval_acc: 0.7460\n",
      "Epoch 12:\ttrain_loss: 0.1735\ttrain_acc: 0.9500\tval_loss: 0.6740\tval_acc: 0.7460\n",
      "Epoch 13:\ttrain_loss: 0.1442\ttrain_acc: 0.9500\tval_loss: 0.6848\tval_acc: 0.7460\n",
      "Epoch 14:\ttrain_loss: 0.1457\ttrain_acc: 0.9500\tval_loss: 0.6912\tval_acc: 0.7460\n",
      "Epoch 15:\ttrain_loss: 0.1398\ttrain_acc: 0.9500\tval_loss: 0.7177\tval_acc: 0.7778\n",
      "Epoch 16:\ttrain_loss: 0.1201\ttrain_acc: 0.9500\tval_loss: 0.7504\tval_acc: 0.7460\n",
      "Epoch 17:\ttrain_loss: 0.0876\ttrain_acc: 1.0000\tval_loss: 0.7872\tval_acc: 0.7937\n",
      "Epoch 18:\ttrain_loss: 0.0875\ttrain_acc: 1.0000\tval_loss: 0.8326\tval_acc: 0.7778\n",
      "Epoch 19:\ttrain_loss: 0.0823\ttrain_acc: 1.0000\tval_loss: 0.8587\tval_acc: 0.7937\n",
      "Epoch 20:\ttrain_loss: 0.0601\ttrain_acc: 1.0000\tval_loss: 0.8890\tval_acc: 0.7937\n",
      "Epoch 21:\ttrain_loss: 0.0619\ttrain_acc: 1.0000\tval_loss: 0.9865\tval_acc: 0.8571\n",
      "Epoch 22:\ttrain_loss: 0.1133\ttrain_acc: 0.9600\tval_loss: 0.9762\tval_acc: 0.8095\n",
      "Epoch 23:\ttrain_loss: 0.1208\ttrain_acc: 0.9500\tval_loss: 0.9164\tval_acc: 0.7143\n",
      "Epoch 24:\ttrain_loss: 0.0840\ttrain_acc: 1.0000\tval_loss: 1.1840\tval_acc: 0.8413\n",
      "Epoch 25:\ttrain_loss: 0.2058\ttrain_acc: 0.9600\tval_loss: 0.8862\tval_acc: 0.6825\n",
      "Epoch 26:\ttrain_loss: 0.2576\ttrain_acc: 0.9200\tval_loss: 0.8849\tval_acc: 0.6984\n",
      "Epoch 27:\ttrain_loss: 0.1768\ttrain_acc: 0.9600\tval_loss: 0.8206\tval_acc: 0.8730\n",
      "Epoch 28:\ttrain_loss: 0.2017\ttrain_acc: 0.9600\tval_loss: 0.7510\tval_acc: 0.7302\n",
      "Epoch 29:\ttrain_loss: 0.0856\ttrain_acc: 1.0000\tval_loss: 0.8923\tval_acc: 0.7460\n",
      "Epoch 30:\ttrain_loss: 0.0803\ttrain_acc: 1.0000\tval_loss: 0.8121\tval_acc: 0.7937\n",
      "Epoch 31:\ttrain_loss: 0.1060\ttrain_acc: 0.9600\tval_loss: 0.7872\tval_acc: 0.7778\n",
      "Epoch 32:\ttrain_loss: 0.0767\ttrain_acc: 1.0000\tval_loss: 0.8493\tval_acc: 0.8254\n",
      "Epoch 33:\ttrain_loss: 0.0772\ttrain_acc: 1.0000\tval_loss: 0.9254\tval_acc: 0.8413\n",
      "Epoch 34:\ttrain_loss: 0.0640\ttrain_acc: 1.0000\tval_loss: 0.9610\tval_acc: 0.8413\n",
      "Epoch 35:\ttrain_loss: 0.0865\ttrain_acc: 0.9600\tval_loss: 1.0123\tval_acc: 0.7619\n",
      "Epoch 36:\ttrain_loss: 0.0621\ttrain_acc: 1.0000\tval_loss: 1.0129\tval_acc: 0.7937\n",
      "Epoch 37:\ttrain_loss: 0.0249\ttrain_acc: 1.0000\tval_loss: 1.1436\tval_acc: 0.8571\n",
      "Epoch 38:\ttrain_loss: 0.0640\ttrain_acc: 1.0000\tval_loss: 1.1773\tval_acc: 0.7778\n",
      "Epoch 39:\ttrain_loss: 0.0279\ttrain_acc: 1.0000\tval_loss: 1.1712\tval_acc: 0.8413\n",
      "Epoch 40:\ttrain_loss: 0.0169\ttrain_acc: 1.0000\tval_loss: 1.1209\tval_acc: 0.8254\n",
      "Epoch 41:\ttrain_loss: 0.0155\ttrain_acc: 1.0000\tval_loss: 1.5981\tval_acc: 0.7302\n",
      "Epoch 42:\ttrain_loss: 0.0072\ttrain_acc: 1.0000\tval_loss: 1.3419\tval_acc: 0.7778\n",
      "Epoch 43:\ttrain_loss: 0.0070\ttrain_acc: 1.0000\tval_loss: 1.8809\tval_acc: 0.7302\n",
      "Epoch 44:\ttrain_loss: 0.0027\ttrain_acc: 1.0000\tval_loss: 1.6603\tval_acc: 0.7778\n",
      "Epoch 45:\ttrain_loss: 0.0032\ttrain_acc: 1.0000\tval_loss: 3.4989\tval_acc: 0.7619\n",
      "Epoch 46:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.7024\tval_acc: 0.7460\n",
      "Epoch 47:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 4.8313\tval_acc: 0.6984\n",
      "Epoch 48:\ttrain_loss: 0.0006\ttrain_acc: 1.0000\tval_loss: 3.3680\tval_acc: 0.8095\n",
      "Epoch 49:\ttrain_loss: 0.0020\ttrain_acc: 1.0000\tval_loss: 1.4200\tval_acc: 0.6825\n",
      "Epoch 50:\ttrain_loss: 0.0455\ttrain_acc: 1.0000\tval_loss: 1.1775\tval_acc: 0.7937\n",
      "Epoch 51:\ttrain_loss: 0.0530\ttrain_acc: 1.0000\tval_loss: 1.1476\tval_acc: 0.7937\n",
      "Epoch 52:\ttrain_loss: 0.0891\ttrain_acc: 0.9600\tval_loss: 0.9274\tval_acc: 0.8095\n",
      "Epoch 53:\ttrain_loss: 0.0696\ttrain_acc: 1.0000\tval_loss: 0.7738\tval_acc: 0.8730\n",
      "Epoch 54:\ttrain_loss: 0.0691\ttrain_acc: 1.0000\tval_loss: 0.7394\tval_acc: 0.8730\n",
      "Epoch 55:\ttrain_loss: 0.0668\ttrain_acc: 1.0000\tval_loss: 0.7985\tval_acc: 0.8095\n",
      "Epoch 56:\ttrain_loss: 0.0455\ttrain_acc: 1.0000\tval_loss: 0.9313\tval_acc: 0.7619\n",
      "Epoch 57:\ttrain_loss: 0.0324\ttrain_acc: 1.0000\tval_loss: 1.0332\tval_acc: 0.7619\n",
      "Epoch 58:\ttrain_loss: 0.0230\ttrain_acc: 1.0000\tval_loss: 1.1155\tval_acc: 0.8095\n",
      "Epoch 59:\ttrain_loss: 0.0160\ttrain_acc: 1.0000\tval_loss: 1.2266\tval_acc: 0.7937\n",
      "Epoch 60:\ttrain_loss: 0.0081\ttrain_acc: 1.0000\tval_loss: 1.3380\tval_acc: 0.7937\n",
      "Epoch 61:\ttrain_loss: 0.0044\ttrain_acc: 1.0000\tval_loss: 1.4118\tval_acc: 0.8095\n",
      "Epoch 62:\ttrain_loss: 0.0028\ttrain_acc: 1.0000\tval_loss: 1.4704\tval_acc: 0.8095\n",
      "Epoch 63:\ttrain_loss: 0.0020\ttrain_acc: 1.0000\tval_loss: 2.8547\tval_acc: 0.8095\n",
      "Epoch 64:\ttrain_loss: 0.0015\ttrain_acc: 1.0000\tval_loss: 2.9060\tval_acc: 0.8095\n",
      "Epoch 65:\ttrain_loss: 0.0011\ttrain_acc: 1.0000\tval_loss: 2.9520\tval_acc: 0.8095\n",
      "Epoch 66:\ttrain_loss: 0.0009\ttrain_acc: 1.0000\tval_loss: 2.9894\tval_acc: 0.8095\n",
      "Epoch 67:\ttrain_loss: 0.0007\ttrain_acc: 1.0000\tval_loss: 3.0196\tval_acc: 0.8095\n",
      "Epoch 68:\ttrain_loss: 0.0006\ttrain_acc: 1.0000\tval_loss: 3.0458\tval_acc: 0.8095\n",
      "Epoch 69:\ttrain_loss: 0.0005\ttrain_acc: 1.0000\tval_loss: 3.0701\tval_acc: 0.8095\n",
      "Epoch 70:\ttrain_loss: 0.0004\ttrain_acc: 1.0000\tval_loss: 3.0934\tval_acc: 0.8095\n",
      "Epoch 71:\ttrain_loss: 0.0004\ttrain_acc: 1.0000\tval_loss: 3.1153\tval_acc: 0.8095\n",
      "Epoch 72:\ttrain_loss: 0.0003\ttrain_acc: 1.0000\tval_loss: 3.1359\tval_acc: 0.8095\n",
      "Epoch 73:\ttrain_loss: 0.0003\ttrain_acc: 1.0000\tval_loss: 3.1552\tval_acc: 0.8095\n",
      "Epoch 74:\ttrain_loss: 0.0003\ttrain_acc: 1.0000\tval_loss: 3.1733\tval_acc: 0.8095\n",
      "Epoch 75:\ttrain_loss: 0.0003\ttrain_acc: 1.0000\tval_loss: 3.1904\tval_acc: 0.8095\n",
      "Epoch 76:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 3.2066\tval_acc: 0.8095\n",
      "Epoch 77:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 3.2222\tval_acc: 0.8095\n",
      "Epoch 78:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 3.2373\tval_acc: 0.8095\n",
      "Epoch 79:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 3.2521\tval_acc: 0.8095\n",
      "Epoch 80:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 3.2663\tval_acc: 0.8095\n",
      "Epoch 81:\ttrain_loss: 0.0002\ttrain_acc: 1.0000\tval_loss: 3.2798\tval_acc: 0.8095\n",
      "Epoch 82:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.2929\tval_acc: 0.8095\n",
      "Epoch 83:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3058\tval_acc: 0.8095\n",
      "Epoch 84:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3187\tval_acc: 0.8095\n",
      "Epoch 85:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3314\tval_acc: 0.8095\n",
      "Epoch 86:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3438\tval_acc: 0.8095\n",
      "Epoch 87:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3558\tval_acc: 0.8095\n",
      "Epoch 88:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3674\tval_acc: 0.8095\n",
      "Epoch 89:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3790\tval_acc: 0.8095\n",
      "Epoch 90:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.3903\tval_acc: 0.8095\n",
      "Epoch 91:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4017\tval_acc: 0.8095\n",
      "Epoch 92:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4128\tval_acc: 0.8095\n",
      "Epoch 93:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4233\tval_acc: 0.8095\n",
      "Epoch 94:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4340\tval_acc: 0.8095\n",
      "Epoch 95:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4447\tval_acc: 0.8095\n",
      "Epoch 96:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4549\tval_acc: 0.8095\n",
      "Epoch 97:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4651\tval_acc: 0.8095\n",
      "Epoch 98:\ttrain_loss: 0.0001\ttrain_acc: 1.0000\tval_loss: 3.4747\tval_acc: 0.8095\n",
      "Epoch 99:\ttrain_loss: 0.0000\ttrain_acc: 1.0000\tval_loss: 3.4849\tval_acc: 0.8095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Network(input_size=x_train.shape[1], output_size=1).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.network.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "model.train()\n",
    "wandb.watch(model)\n",
    "model.fit(X=x_train, y=y_train, X_val=x_val, y_val=y_val, n_epochs=100, loss_fn=loss_fn, optimizer=optimizer, batch_size=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
